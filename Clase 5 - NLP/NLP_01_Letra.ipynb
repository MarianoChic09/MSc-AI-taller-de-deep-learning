{"cells":[{"cell_type":"markdown","metadata":{"id":"7VMtkNDwxZ0m"},"source":["# Datos\n","\n","Vamos a usar el dataset de IMDB para clasificación de reseñas de películas, el objetivo del mismo es detectar si una reseña tiene sentimiento **positivo** o **negativo**.\n","\n","Descarguen el dataset de este [link](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n","\n","Y word2vec de este [link](https://drive.google.com/file/d/1XusPRjsCVcIdCQ2hQDDWcH_wayfn4nWb/view?usp=sharing).\n","\n","-> Para correr esta notebook en colab suban los archivos a una carpeta **data** en la raiz de su drive personal.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37098,"status":"ok","timestamp":1632863784197,"user":{"displayName":"Matias Sorozabal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04842478827197692237"},"user_tz":180},"id":"zh7A9mjs3cfU","outputId":"b235382f-0109-4101-849a-d91c44fbac9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"," drive\t'IMDB Dataset.csv'   sample_data   word2vec.txt\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","! cp \"/content/drive/My Drive/data/IMDB_Dataset.zip\" .\n","! unzip -q IMDB_Dataset.zip\n","! rm IMDB_Dataset.zip\n","! ls"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"BBaFc6ZZzp2T"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['review', 'sentiment'], dtype='object')\n"]}],"source":["import pandas as pd\n","imdb_data = pd.read_csv(\"IMDB Dataset.csv\")\n","\n","#sentiment count\n","print(imdb_data.columns)\n","imdb_data['sentiment'].value_counts()\n","\n","# Convert positive and negative into binary classes (1-0)\n","from sklearn.preprocessing import LabelBinarizer\n","lb = LabelBinarizer()\n","\n","sentiment_data = lb.fit_transform(imdb_data[\"sentiment\"])\n","imdb_data['sentiment'] = sentiment_data"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"rQFZbEtC1T94"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Probably my all-time favorite movie, a story o...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>I sure would like to see a resurrection of a u...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>This show was an amazing, fresh &amp; innovative i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Encouraged by the positive comments about this...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>If you like original gut wrenching laughter yo...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  sentiment\n","0  One of the other reviewers has mentioned that ...          1\n","1  A wonderful little production. <br /><br />The...          1\n","2  I thought this was a wonderful way to spend ti...          1\n","3  Basically there's a family where a little boy ...          0\n","4  Petter Mattei's \"Love in the Time of Money\" is...          1\n","5  Probably my all-time favorite movie, a story o...          1\n","6  I sure would like to see a resurrection of a u...          1\n","7  This show was an amazing, fresh & innovative i...          0\n","8  Encouraged by the positive comments about this...          0\n","9  If you like original gut wrenching laughter yo...          1"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["imdb_data.head(10)"]},{"cell_type":"markdown","metadata":{"id":"rhdlJtOo2YC-"},"source":["# Imports\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in c:\\users\\maria\\anaconda3\\envs\\mario_env_windows\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n","\n","Requirement already satisfied: click in c:\\users\\maria\\anaconda3\\envs\\mario_env_windows\\lib\\site-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in c:\\users\\maria\\anaconda3\\envs\\mario_env_windows\\lib\\site-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\maria\\anaconda3\\envs\\mario_env_windows\\lib\\site-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in c:\\users\\maria\\anaconda3\\envs\\mario_env_windows\\lib\\site-packages (from nltk) (4.65.0)\n","Requirement already satisfied: colorama in c:\\users\\maria\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"]}],"source":["%pip install nltk\n","#,bs4"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"YaxC7dfq2aKT"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\maria\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","from bs4 import BeautifulSoup\n","\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"markdown","metadata":{"id":"Lkr7Vz950IeY"},"source":["# Preprocesamiento Inicial\n","\n","Como toda tarea de NLP tenemos que comenzar preprocesando los datos, eliminando palabras que no nos sirve, caracteres especiales, etc.\n","\n","En particular hay tres tareas a ser realizadas basadas en un análisis inicial del dataset (mirando ejemplos al azar del mismo)\n","\n","\n","\n","1.   Eliminar tags html (vamos a utilizar BeautifulSoup para esto)\n","2.   Eliminar texto entre parentesis rectos (Usando la siguiente expresion regular: ```\\[[^]]*\\]``` )\n","3. Eliminar caracteres especiales, usando una regex quitar todos los caracteres que no son ni letras ni números (```[^a-zA-z0-9\\s] ``` )\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["![Alt text](image.png)"]},{"cell_type":"markdown","metadata":{},"source":["![Alt text](image-1.png)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"SIiaBeGOwIOQ"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_11760\\1373846029.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  soup = BeautifulSoup(text,\"html.parser\")\n"]}],"source":["def strip_html(text):\n","  soup = BeautifulSoup(text,\"html.parser\")\n","  return soup.get_text().strip()\n","\n","def remove_between_square_brackets(text):\n","  p = re.compile('\\[[^]]*\\]')\n","  return p.sub(' ', text)\n","\n","def remove_special_characters(text):\n","  p = re.compile('[^a-zA-Z0-9 ]')\n","  return p.sub(' ', text)\n","\n","def low_level_preproc(text):\n","  return remove_special_characters(remove_between_square_brackets(strip_html(text)))\n","\n","#Apply function on review column\n","imdb_data['review'] = imdb_data['review'].apply(low_level_preproc)"]},{"cell_type":"markdown","metadata":{"id":"gGJUuc0O16iv"},"source":["# Preprocesamiento de alto nivel\n","\n","Una vez tenemos el texto limpio y trabajable volvemos a hacer otro pasaje de preprocesamiento de más alto nivel, ahora vamos a querer:\n","\n","\n","\n","1.   Transformar todo el texto a minúscula\n","2.   Quitar stop words (usando nltk)\n","3.   Lemmatizar usando nltk WordNetLemmatizer\n","\n","Para todo esto vamos a necesitar trabajar con **tokens** palabras individuales, en este caso vamos a separar por **whitespace**, pero se podrían usar mejores estrategias.\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"KeKNTxvzwISR"},"outputs":[],"source":["all_stopwords = set(stopwords.words(\"english\"))\n","\n","def remove_stop_words(full_text_line):\n","  tokens = full_text_line.split(\" \")\n","  valid_tokens = [word for word in tokens if word not in all_stopwords]\n","  return valid_tokens\n","\n","def lemmatize(tokens):\n","  lemmatizer = WordNetLemmatizer()\n","  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","  return lemmatized_tokens\n","\n","def high_level_preproc(text):\n","  return \" \".join(lemmatize(remove_stop_words(text)))\n","\n","#Apply function on review column\n","imdb_data['review'] = imdb_data['review'].str.lower()\n","imdb_data['review'] = imdb_data['review'].apply(high_level_preproc)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"9k3wz_XUql1d"},"outputs":[{"data":{"text/plain":["0    one reviewer mentioned watching 1 oz episode h...\n","1    wonderful little production  filming technique...\n","2    thought wonderful way spend time hot summer we...\n","3    basically family little boy  jake  think zombi...\n","4    petter mattei  love time money  visually stunn...\n","5    probably time favorite movie  story selflessne...\n","6    sure would like see resurrection dated seahunt...\n","7    show amazing  fresh   innovative idea 70 first...\n","8    encouraged positive comment film looking forwa...\n","9    like original gut wrenching laughter like movi...\n","Name: review, dtype: object"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["imdb_data['review'].head(10)"]},{"cell_type":"markdown","metadata":{"id":"MAynpGKc3dYn"},"source":["# Modelando\n","\n","Para modelar vamos a comenzar separando el dataset."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"OFZn34Q63lLE"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train set: (40000,) (40000,)\n","Test set: (10000,) (10000,)\n"]}],"source":["#split the dataset  \n","#train dataset\n","train_reviews = imdb_data.review[:40000]\n","train_sentiments = imdb_data.sentiment[:40000]\n","\n","#test dataset\n","test_reviews = imdb_data.review[40000:]\n","test_sentiments = imdb_data.sentiment[40000:]\n","\n","\n","print(\"Train set:\", train_reviews.shape, train_sentiments.shape)\n","print(\"Test set:\", test_reviews.shape, test_sentiments.shape)"]},{"cell_type":"markdown","metadata":{"id":"U16zBbGzAb5z"},"source":["Vamos a generar vectores para las reseñas usando TF-IDF (sklearn). Vamos a hacer uso del parametro ```max_features``` que nos permite controlar cuántas palabras considerar para generar los vectores (en orden de frecuencia). Luego usamos esa representacion vectorial para entrenar y testear un regresor logístico (LogisticRegressor). En particular vamos a empezar con 300 features, más adelante veremos por qué.\n","\n","\n","Vamos a entrenar el modelo por 500 iteraciones como máximo y usamos l2 como regularizador."]},{"cell_type":"markdown","metadata":{},"source":["![Alt text](image-3.png)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"9RVouRZ_ql3I"},"outputs":[{"data":{"text/plain":["array(['10', 'able', 'absolutely', 'across', 'act', 'acting', 'action',\n","       'actor', 'actress', 'actually', 'add', 'age', 'ago', 'almost',\n","       'alone', 'along', 'already', 'also', 'although', 'always',\n","       'amazing', 'american', 'another', 'anyone', 'anything', 'anyway',\n","       'around', 'art', 'attempt', 'audience', 'away', 'awful', 'back',\n","       'bad', 'based', 'beautiful', 'become', 'becomes', 'begin',\n","       'beginning', 'behind', 'believe', 'best', 'better', 'big', 'bit',\n","       'black', 'blood', 'body', 'book', 'boring', 'boy', 'brilliant',\n","       'brother', 'budget', 'call', 'called', 'came', 'camera', 'cannot',\n","       'car', 'care', 'career', 'case', 'cast', 'certainly', 'chance',\n","       'change', 'character', 'child', 'cinema', 'city', 'classic',\n","       'close', 'come', 'comedy', 'coming', 'comment', 'complete',\n","       'completely', 'cool', 'cop', 'could', 'country', 'couple',\n","       'course', 'crap', 'credit', 'cut', 'dark', 'daughter', 'david',\n","       'day', 'dead', 'deal', 'death', 'decent', 'definitely', 'despite',\n","       'dialogue', 'different', 'directed', 'direction', 'director',\n","       'documentary', 'dog', 'done', 'drama', 'dvd', 'early', 'effect',\n","       'effort', 'either', 'element', 'else', 'end', 'ending', 'english',\n","       'enjoy', 'enjoyed', 'enough', 'entertaining', 'entire', 'episode',\n","       'especially', 'etc', 'even', 'event', 'ever', 'every', 'everyone',\n","       'everything', 'evil', 'exactly', 'example', 'excellent', 'except',\n","       'expect', 'experience', 'extremely', 'eye', 'face', 'fact', 'fall',\n","       'family', 'fan', 'far', 'father', 'favorite', 'feature', 'feel',\n","       'feeling', 'felt', 'female', 'fight', 'film', 'final', 'finally',\n","       'find', 'fine', 'first', 'flick', 'found', 'friend', 'full', 'fun',\n","       'funny', 'game', 'gave', 'genre', 'get', 'getting', 'girl', 'give',\n","       'given', 'go', 'god', 'going', 'good', 'gore', 'got', 'great',\n","       'group', 'guess', 'guy', 'half', 'hand', 'happen', 'happened',\n","       'happens', 'hard', 'head', 'heard', 'heart', 'hell', 'help',\n","       'hero', 'high', 'highly', 'hilarious', 'history', 'hit',\n","       'hollywood', 'home', 'hope', 'horrible', 'horror', 'hour', 'house',\n","       'however', 'human', 'humor', 'husband', 'idea', 'including',\n","       'instead', 'interest', 'interesting', 'involved', 'james', 'job',\n","       'john', 'joke', 'keep', 'kid', 'kill', 'killed', 'killer', 'kind',\n","       'know', 'known', 'lack', 'lady', 'last', 'late', 'later', 'laugh',\n","       'le', 'lead', 'least', 'leave', 'left', 'let', 'level', 'life',\n","       'light', 'like', 'liked', 'line', 'little', 'live', 'living',\n","       'long', 'look', 'looked', 'looking', 'lost', 'lot', 'love',\n","       'loved', 'low', 'made', 'main', 'make', 'making', 'man', 'many',\n","       'matter', 'may', 'maybe', 'mean', 'meet', 'men', 'michael',\n","       'might', 'mind', 'minute', 'miss', 'moment', 'money', 'monster',\n","       'mother', 'move', 'movie', 'mr', 'much', 'murder', 'music',\n","       'musical', 'must', 'name', 'need', 'never', 'new', 'next', 'nice',\n","       'night', 'nothing', 'novel', 'number', 'obvious', 'obviously',\n","       'often', 'oh', 'ok', 'old', 'one', 'opening', 'opinion', 'order',\n","       'original', 'others', 'overall', 'part', 'particularly', 'past',\n","       'people', 'perfect', 'performance', 'perhaps', 'person', 'picture',\n","       'piece', 'place', 'play', 'played', 'playing', 'please', 'plot',\n","       'point', 'police', 'poor', 'possible', 'power', 'pretty',\n","       'probably', 'problem', 'production', 'put', 'quality', 'question',\n","       'quite', 'rather', 'rating', 'read', 'real', 'reality', 'really',\n","       'reason', 'recommend', 'relationship', 'released', 'remember',\n","       'rest', 'review', 'right', 'robert', 'role', 'room', 'run',\n","       'running', 'said', 'save', 'saw', 'say', 'saying', 'scene',\n","       'school', 'score', 'screen', 'script', 'second', 'see', 'seeing',\n","       'seem', 'seemed', 'seems', 'seen', 'self', 'sense', 'sequel',\n","       'sequence', 'series', 'serious', 'seriously', 'set', 'several',\n","       'sex', 'short', 'shot', 'show', 'shown', 'side', 'simple',\n","       'simply', 'since', 'sister', 'situation', 'slow', 'small',\n","       'someone', 'something', 'sometimes', 'son', 'song', 'soon', 'sort',\n","       'sound', 'special', 'stand', 'star', 'start', 'still', 'stop',\n","       'story', 'strong', 'stuff', 'stupid', 'style', 'supposed', 'sure',\n","       'take', 'taken', 'talent', 'talk', 'tell', 'terrible', 'theater',\n","       'theme', 'thing', 'think', 'thinking', 'though', 'thought',\n","       'three', 'thriller', 'throughout', 'time', 'title', 'today',\n","       'together', 'told', 'took', 'top', 'totally', 'town', 'true',\n","       'truly', 'try', 'trying', 'turn', 'tv', 'twist', 'two', 'type',\n","       'understand', 'unfortunately', 'use', 'used', 'value', 'version',\n","       'video', 'view', 'viewer', 'violence', 'voice', 'want', 'wanted',\n","       'war', 'waste', 'watch', 'watched', 'watching', 'way', 'well',\n","       'went', 'white', 'whole', 'wife', 'wish', 'without', 'woman',\n","       'wonder', 'wonderful', 'word', 'work', 'world', 'worse', 'worst',\n","       'worth', 'would', 'writer', 'writing', 'written', 'wrong', 'year',\n","       'yes', 'yet', 'young', 'zombie'], dtype=object)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# Su código para vectorizar\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(max_features=500)\n","vectorizer.fit(train_reviews)\n","X_train = vectorizer.transform(train_reviews)\n","vectorizer.get_feature_names_out()"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["X_train = vectorizer.transform(train_reviews)\n","X_test = vectorizer.transform(test_reviews)"]},{"cell_type":"markdown","metadata":{},"source":["![Alt text](image-4.png)"]},{"cell_type":"markdown","metadata":{},"source":["![Alt text](image-5.png)"]},{"cell_type":"markdown","metadata":{},"source":["### Su código para el modelo\n"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"SCOsCtrEAaJj"},"outputs":[],"source":["clf = LogisticRegression(random_state=0).fit(X_train, train_sentiments)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["0.8427"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["y_hat = clf.predict(X_test)\n","accuracy_score(test_sentiments, y_hat)"]},{"cell_type":"markdown","metadata":{"id":"btnSQOb2JwIL"},"source":["# Vectores pre entrenados\n","Ahora vamos a ver si podemos superar la performance del modelo haciendo uso de deep learning.  Primero vamos a entrenar el mismo modelo usando los embeddings preentrenados de Word2Vec (usando gensim). \n","\n","Luego vamos a darle esos embeddings a un MLP y ver si logramos superar la performance anterior."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYqJ0uJ1XH6Y"},"outputs":[],"source":["import gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7oiP209Jvs1"},"outputs":[],"source":["w2v = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec.txt\", binary=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ap22Pw93U586"},"outputs":[],"source":["mean_vector = np.mean(w2v.vectors, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVwCEMusJvw0"},"outputs":[],"source":["def get_sentence_embedding(text):\n","  pass\n","\n","\n","train_vectors = [get_sentence_embedding(sent) for sent in train_reviews]\n","test_vectors = [get_sentence_embedding(sent) for sent in test_reviews]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"884gkcqLVvyL"},"outputs":[],"source":["# Su código para el modelo (Igual que antes)"]},{"cell_type":"markdown","metadata":{"id":"-2GvqL87Yipe"},"source":["# Deep Learning\n"]},{"cell_type":"markdown","metadata":{"id":"5oxjMLJAW8qr"},"source":["MLP: vamos a crear un MLP para atacar ese mismo problema, el diseño corre por su cuenta pero deberían ser capaces de obetener mejor performance en test que los modelos anteriores.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqH91xLsYlQr"},"outputs":[],"source":["# Imports\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(DEVICE)\n","\n","torch.manual_seed(42)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUuaAbANZscl"},"outputs":[],"source":["def train_epoch(training_model, loader, criterion, optim):\n","    training_model.train()\n","    epoch_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    for data, labels in loader:\n","      all_labels.extend(labels.numpy())  \n","\n","      optim.zero_grad()\n","\n","      predictions = training_model(data.to(DEVICE))\n","      all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","\n","      loss = criterion(predictions, labels.to(DEVICE))\n","      \n","      loss.backward()\n","      optim.step()\n","\n","      epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n","\n","\n","def validation_epoch(val_model, loader, criterion):\n","    val_model.eval()\n","    epoch_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    with torch.no_grad():\n","      for data, labels in loader:\n","        all_labels.extend(labels.numpy())  \n","\n","        predictions = val_model(data.to(DEVICE))\n","        all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","\n","        loss = criterion(predictions, labels.to(DEVICE))\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n","  \n","\n","def train_model(model, train_loader, test_loader, criterion, optim, number_epochs):\n","  train_history = []\n","  test_history = []\n","  accuracy_history = []\n","\n","  for epoch in range(number_epochs):\n","      start_time = time.time()\n","\n","      train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n","      train_history.append(train_loss)\n","      print(\"Training epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n","            .format(epoch + 1, train_loss, train_acc, time.time() - start_time))\n","\n","      start_time = time.time()\n","      test_loss, acc = validation_epoch(model, test_loader, criterion)\n","      test_history.append(test_loss)\n","      accuracy_history.append(acc)\n","      print(\"Validation epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n","            .format(epoch + 1, test_loss, acc, time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M46rdl18JvzG"},"outputs":[],"source":["class MLP(nn.Module):\n","\n","  def __init__(self, in_features):\n","    super(MLP, self).__init__()\n","    # Su implementacion\n","\n","  def forward(self, new_input):\n","    # Su implementacion\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wl3PabnAX4_1"},"outputs":[],"source":["loss_function = nn.CrossEntropyLoss().to(DEVICE)\n","modelo = MLP(in_features=300).to(DEVICE)\n","optimizer = torch.optim.Adam(modelo.parameters(), lr=0.001)\n","BATCH_SIZE = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCauiKoIX5CJ"},"outputs":[],"source":["# Dataloaders\n","train_vectors = [get_sentence_embedding(sent) for sent in train_reviews]\n","test_vectors = [get_sentence_embedding(sent) for sent in test_reviews]\n","\n","train_targets = torch.Tensor(train_sentiments.to_numpy()).long()\n","train_dataset = TensorDataset(torch.Tensor(train_vectors), train_targets) \n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=2)\n","\n","test_targets = torch.Tensor(test_sentiments.to_numpy()).long()\n","test_dataset = TensorDataset(torch.Tensor(test_vectors), test_targets) \n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDqPZnJgZWDi"},"outputs":[],"source":["train_model(modelo, train_dataloader, test_dataloader, loss_function, optimizer, 10)"]},{"cell_type":"markdown","metadata":{"id":"zBfOKG3iMt9Q"},"source":["# Exploración\n","\n","Exploren otras técnicas de preprocesamiento, tokenizacion, vectorizacion, etc. para ver si puede lograr superar los modelos presentados en clase.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T09IUvPVJv06"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["zBfOKG3iMt9Q"],"name":"NLP_01_Letra.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
