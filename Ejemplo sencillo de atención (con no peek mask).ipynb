{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarianoChic09/MSc-AI-taller-de-deep-learning/blob/main/Ejemplo%20sencillo%20de%20atenci%C3%B3n%20(con%20no%20peek%20mask).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WnSipAduBeFz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMveK14wBeF1"
      },
      "source": [
        "# Mecanismo de atencion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlCPR7nABeF3"
      },
      "source": [
        "![Attention diagram](https://miro.medium.com/max/336/1*15E9qKg9bKnWdSRWCyY2iA.png)\n",
        "![Attention equation](https://miro.medium.com/max/1068/1*evdACdTOBT5j1g1nXialBg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN46Pu0dBeF3"
      },
      "source": [
        "Ignorando la noción de multihead attention y el batch size (por ahora) podemos imaginar un dato de dimensiones (seq_len, d_model)\n",
        "\n",
        "Dicho dato, es procesado por 3 perceptrones independientes que llamaremos pQ, pK y pV, con dimensión de entrada y salida d_model, a efectos de simplificar el ejemplo asumiremos que son la función identidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZT3Rt1uWBeF3",
        "outputId": "778a306a-5f67-469b-83f1-9b2d9b09c7e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
            "         0.0900],\n",
            "        [0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700, 0.1800,\n",
            "         0.1900],\n",
            "        [0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600, 0.2700, 0.2800,\n",
            "         0.2900],\n",
            "        [0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500, 0.3600, 0.3700, 0.3800,\n",
            "         0.3900],\n",
            "        [0.4000, 0.4100, 0.4200, 0.4300, 0.4400, 0.4500, 0.4600, 0.4700, 0.4800,\n",
            "         0.4900]])\n",
            "torch.Size([5, 10])\n"
          ]
        }
      ],
      "source": [
        "seq_len = 5\n",
        "d_model = 10\n",
        "dato = torch.tensor(range(d_model*seq_len), dtype=torch.float32).view(seq_len, d_model)/100\n",
        "print(dato)\n",
        "print(dato.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-_aXtDc_BeF4"
      },
      "outputs": [],
      "source": [
        "Q = K = V = dato"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrQMoi4TBeF4"
      },
      "source": [
        "El primer paso de la atención es multiplicar Q por K transpuesta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MGX0hmcwBeF5",
        "outputId": "d9ee2ab1-b403-4b79-b50e-de8f38a3abf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0285, 0.0735, 0.1185, 0.1635, 0.2085],\n",
              "        [0.0735, 0.2185, 0.3635, 0.5085, 0.6535],\n",
              "        [0.1185, 0.3635, 0.6085, 0.8535, 1.0985],\n",
              "        [0.1635, 0.5085, 0.8535, 1.1985, 1.5435],\n",
              "        [0.2085, 0.6535, 1.0985, 1.5435, 1.9885]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "QKt = torch.matmul(Q, K.transpose(-2, -1))\n",
        "QKt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsveFDgdBeF5"
      },
      "source": [
        "Si pensamos como marco de referencia nuestro dato, e interpretamos su ultimo valor como el encoding asociado al símbolo de padding sería deseable que la posición 5 del valor retornado por la atención no tenga aporte.\n",
        "\n",
        "Para ello podemos crear una máscara de padding con un 0 en la posición 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CPShWM2RBeF5"
      },
      "outputs": [],
      "source": [
        "mask = torch.tensor([1,1,1,1,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ypbozgFBeF5"
      },
      "source": [
        "Aplicar la máscara en este caso es sencillo"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TcxrTXssFYk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_nopeak_mask(size):\n",
        "  nopeak_mask = torch.triu(torch.ones(size,size)).transpose(0,1)\n",
        "  return nopeak_mask.unsqueeze(0).type(torch.float32)"
      ],
      "metadata": {
        "id": "tZOBjRyKEr2V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_peak = build_nopeak_mask(5)"
      ],
      "metadata": {
        "id": "MP99DjitE8D8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_peak_mask_applied = pad_mask_applied.masked_fill(no_peak ==0, -1e-9).squeeze(0)"
      ],
      "metadata": {
        "id": "4wEADjn4FmGd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_peak_mask_applied"
      ],
      "metadata": {
        "id": "-VvONP3qHsTJ",
        "outputId": "78a65d68-b93c-4427-98f4-187306fb2e6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.8500e-02, -1.0000e-09, -1.0000e-09, -1.0000e-09, -1.0000e-09],\n",
              "        [ 7.3500e-02,  2.1850e-01, -1.0000e-09, -1.0000e-09, -1.0000e-09],\n",
              "        [ 1.1850e-01,  3.6350e-01,  6.0850e-01, -1.0000e-09, -1.0000e-09],\n",
              "        [ 1.6350e-01,  5.0850e-01,  8.5350e-01,  1.1985e+00, -1.0000e-09],\n",
              "        [ 2.0850e-01,  6.5350e-01,  1.0985e+00,  1.5435e+00, -1.0000e+09]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwGSbUjpBeF6"
      },
      "source": [
        "En el tensor resultante, las filas (i) representan palabras, y las columnas (j) también, donde se expresa que para cada i hay una relación con j, donde para el caso j = 4 este debe ser ignorado por los i pues este elemento corresponde al padding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCQhgE2_BeF6"
      },
      "source": [
        "El paso siguiente es ponderar estos resultados en un valor entre 0 y 1, pudiendo interpretarse dicho valor como la importancia de la palbra j para la palabra i."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zqLzWC_xBeF6",
        "outputId": "99a8ff74-2a0e-4292-93dc-e6209b59e503",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-6f9094d5a48e>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  importances = F.softmax(no_peak_mask_applied)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2046, 0.1989, 0.1989, 0.1989, 0.1989],\n",
              "        [0.2023, 0.2339, 0.1880, 0.1880, 0.1880],\n",
              "        [0.1759, 0.2247, 0.2871, 0.1562, 0.1562],\n",
              "        [0.1239, 0.1750, 0.2471, 0.3488, 0.1052],\n",
              "        [0.1137, 0.1774, 0.2769, 0.4320, 0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "importances = F.softmax(no_peak_mask_applied)\n",
        "\n",
        "importances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t-WDuwABeF6"
      },
      "source": [
        "La siguiente matriz nos dice por ejemplo que el aporte al valor final para la palabra 2 dado por la palabra 3 es aproximadamente 35%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS36IzpSBeF6",
        "outputId": "152d0312-0e61-4482-cc71-752ba37e2eca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[23, 24, 26, 27,  0],\n",
              "       [20, 23, 27, 31,  0],\n",
              "       [17, 21, 27, 35,  0],\n",
              "       [14, 20, 28, 39,  0],\n",
              "       [11, 18, 28, 43,  0]], dtype=int32)"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.array(np.round(importances, decimals = 2)*100, dtype = \"int32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1pMiUa0BeF6"
      },
      "source": [
        "Finalmente debemos obtener un resultado para cada palabra de entrada, esto es, un vector que resulta de operar V, con la matriz de importancias."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "importances = F.softmax(no_peak_mask_applied)\n",
        "\n",
        "importances"
      ],
      "metadata": {
        "id": "ZufrbUIGGBQv",
        "outputId": "06022f76-afaf-44f8-a17e-60cc4462b6dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-6f9094d5a48e>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  importances = F.softmax(no_peak_mask_applied)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOj_kcqLBeF7",
        "outputId": "1c479094-ee3a-40f9-f032-3c0e1bd540b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.1556, 0.1656, 0.1756, 0.1856, 0.1956, 0.2056, 0.2156, 0.2256, 0.2356,\n",
              "         0.2456],\n",
              "        [0.1680, 0.1780, 0.1880, 0.1980, 0.2080, 0.2180, 0.2280, 0.2380, 0.2480,\n",
              "         0.2580],\n",
              "        [0.1801, 0.1901, 0.2001, 0.2101, 0.2201, 0.2301, 0.2401, 0.2501, 0.2601,\n",
              "         0.2701],\n",
              "        [0.1917, 0.2017, 0.2117, 0.2217, 0.2317, 0.2417, 0.2517, 0.2617, 0.2717,\n",
              "         0.2817],\n",
              "        [0.2027, 0.2127, 0.2227, 0.2327, 0.2427, 0.2527, 0.2627, 0.2727, 0.2827,\n",
              "         0.2927]])"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = torch.matmul(importances, V)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM0CHNGTBeF7",
        "outputId": "45429448-2cae-4564-9e0e-fe04b961d5d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 10])"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U_SZ18xBeF7"
      },
      "source": [
        "Finalmente tenemos una salida del mismo tamaño que la entrada. Todas estas operaciones se pueden realizar a la misma vez para un batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMN5EKSgBeF7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}