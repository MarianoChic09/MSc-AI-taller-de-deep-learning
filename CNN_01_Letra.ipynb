{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarianoChic09/MSc-AI-taller-de-deep-learning/blob/main/CNN_01_Letra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvzEJ0JjqYnq",
        "outputId": "19111c21-2710-4fff-ae8e-aa2ee714879a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.0-cp311-cp311-win_amd64.whl (9.2 MB)\n",
            "                                              0.0/9.2 MB ? eta -:--:--\n",
            "                                              0.0/9.2 MB ? eta -:--:--\n",
            "                                              0.0/9.2 MB ? eta -:--:--\n",
            "                                              0.0/9.2 MB 187.9 kB/s eta 0:00:49\n",
            "                                              0.0/9.2 MB 196.9 kB/s eta 0:00:47\n",
            "                                              0.1/9.2 MB 374.1 kB/s eta 0:00:25\n",
            "     -                                        0.2/9.2 MB 846.9 kB/s eta 0:00:11\n",
            "     --                                       0.5/9.2 MB 1.7 MB/s eta 0:00:06\n",
            "     ----                                     0.9/9.2 MB 2.5 MB/s eta 0:00:04\n",
            "     -----                                    1.2/9.2 MB 3.0 MB/s eta 0:00:03\n",
            "     ------                                   1.5/9.2 MB 3.3 MB/s eta 0:00:03\n",
            "     -------                                  1.8/9.2 MB 3.7 MB/s eta 0:00:02\n",
            "     ---------                                2.1/9.2 MB 3.8 MB/s eta 0:00:02\n",
            "     ----------                               2.3/9.2 MB 3.9 MB/s eta 0:00:02\n",
            "     -----------                              2.7/9.2 MB 4.2 MB/s eta 0:00:02\n",
            "     -------------                            3.0/9.2 MB 4.4 MB/s eta 0:00:02\n",
            "     --------------                           3.3/9.2 MB 4.5 MB/s eta 0:00:02\n",
            "     ---------------                          3.7/9.2 MB 4.7 MB/s eta 0:00:02\n",
            "     -----------------                        4.0/9.2 MB 4.9 MB/s eta 0:00:02\n",
            "     ------------------                       4.2/9.2 MB 4.7 MB/s eta 0:00:02\n",
            "     -------------------                      4.5/9.2 MB 4.9 MB/s eta 0:00:01\n",
            "     --------------------                     4.8/9.2 MB 5.0 MB/s eta 0:00:01\n",
            "     ----------------------                   5.2/9.2 MB 5.2 MB/s eta 0:00:01\n",
            "     -----------------------                  5.4/9.2 MB 5.1 MB/s eta 0:00:01\n",
            "     ------------------------                 5.7/9.2 MB 5.2 MB/s eta 0:00:01\n",
            "     --------------------------               6.1/9.2 MB 5.3 MB/s eta 0:00:01\n",
            "     ----------------------------             6.5/9.2 MB 5.4 MB/s eta 0:00:01\n",
            "     -----------------------------            6.7/9.2 MB 5.4 MB/s eta 0:00:01\n",
            "     -------------------------------          7.1/9.2 MB 5.6 MB/s eta 0:00:01\n",
            "     --------------------------------         7.5/9.2 MB 5.7 MB/s eta 0:00:01\n",
            "     ---------------------------------        7.8/9.2 MB 5.7 MB/s eta 0:00:01\n",
            "     -----------------------------------      8.1/9.2 MB 5.7 MB/s eta 0:00:01\n",
            "     -------------------------------------    8.6/9.2 MB 5.8 MB/s eta 0:00:01\n",
            "     --------------------------------------   8.8/9.2 MB 5.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  9.0/9.2 MB 5.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  9.2/9.2 MB 5.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 9.2/9.2 MB 5.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\maria\\anaconda3\\envs\\mario_env_windows\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\maria\\anaconda3\\envs\\mario_env_windows\\lib\\site-packages (from scikit-learn) (1.11.0)\n",
            "Collecting joblib>=1.1.1 (from scikit-learn)\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
            "Successfully installed joblib-1.3.2 scikit-learn-1.3.0 threadpoolctl-3.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QqmpXrnMimgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d93182d-454d-4b53-8669-8ca747293bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKQocE-UXzzS"
      },
      "source": [
        "# Redes Convolucionales\n",
        "\n",
        "\n",
        "![Image](https://www.unite.ai/wp-content/uploads/2019/12/Typical_cnn-1.png)\n",
        "\n",
        "\n",
        "Las redes convolucionales (CNNs) se basan en el uso de una tecnica muy usada en el campo de computer vision tradicional, las **convoluciones** (https://en.wikipedia.org/wiki/Kernel_(image_processing), la idea es crear un filtro pequeño que pasamos por encima de toda la imagen y nos permite detectar distintos elementos (como son líneas verticales, horizontales, diagonales, circulos, etc). EL gran problema de las convoluciones es que para crear dichos filtros debemos poder especificar distintos valores (pesos) para cada región en el mismo.\n",
        "\n",
        "Cada filtro (tambien conocido como kernel) nos permite identificar algo en particular en la imágen, y aplicar un filtro al resultado de otro (u otros) nos permite obtener informacion de más alto nivel (como por ejemplo detectar ojos, ruedas, puertas, etc).\n",
        "\n",
        "![Image](https://d2l.ai/_images/correlation.svg)\n",
        "\n",
        "***\n",
        "\n",
        "Las redes convolucionales nos dan una manera de no sólo aprender los vaores óptimos para dichos filtros (mediante backprop) sino tambien la posibilidad de hacerlo a escala usndo un número arbitrario de los mismos. Una gran ventaja que nos trae el uso de filtros, es el hecho de que requieren de un número muy chico de pesos a entrenar, lo que reduce el tamaño de nuestra red y nos permite entrenar mas rápido (o redes mas grandes y profundas con el mismo hardware).\n",
        "\n",
        "Una cosa a notar en las redes convolucionales es el hecho de que las imágenes se van reduciendo en su tamaño a medida que fluyen por la red, esto se debe a la opeación de `maxpooling` que toma regiones (por lo general de 2x2) en nuestra imagen y se queda con el valor más alto en la zona, reduciendo asi el tamaño de la imagen. El resultado de aplicar un filtro de convolución a una imagen se llama `feature_map` y se puede pensar como otra imagen que describe la características de la original.\n",
        "\n",
        "***\n",
        "\n",
        "Al final de nuestra red, necesitamos formar una predicción de la clase de nuestra imagen, por lo que tenemos que \"achatar\" estos feature maps y pasarlos por una (o varias) capas lineales que generen una predicción. Esto se puede ver como representar toda la informacion que conocemos de la imagen, como por ejemplo si tiene nariz, orejas, pelo, en un sólo vector; y decidir que ese vector representa a un perro.\n",
        "\n",
        "***\n",
        "\n",
        "Para empezar, volvemos a definir nuestros conjuntos de datos. Esta vez, sin hacer uso de ninguna transformacion sobre la imagen (mas que transformarla en un tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3vQERzsOXzzT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4936217e-ebf5-4159-8440-248664bdf796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 10517438.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ruta_donde_guardar_datos/FashionMNIST/raw/train-images-idx3-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 174541.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ruta_donde_guardar_datos/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3236179.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ruta_donde_guardar_datos/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 21378492.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ruta_donde_guardar_datos/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ruta_donde_guardar_datos/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Descargamos los datasets\n",
        "# fashion_mnist_train_dataset = datasets.FashionMNIST(\n",
        "#     \"ruta_donde_guardar_datos\",\n",
        "#     download=True,\n",
        "#     train=True,\n",
        "#     transform=transform,\n",
        "# )\n",
        "\n",
        "mnist_train_dataset = datasets.FashionMNIST(\n",
        "    \"ruta_donde_guardar_datos\",\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "# Separamos el train set en train y validation\n",
        "train_set, val_set = torch.utils.data.random_split(\n",
        "    mnist_train_dataset,\n",
        "    [int(0.8 * len(mnist_train_dataset)), int(0.2 * len(mnist_train_dataset))],\n",
        ")\n",
        "\n",
        "mnist_test_dataset = datasets.FashionMNIST(\n",
        "    \"ruta_donde_guardar_datos\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "# Creamos objetos DataLoader (https://pytorch.org/docs/stable/data.html) que nos va a permitir crear batches de data automaticamente.\n",
        "\n",
        "# Cuantas imagenes obtener en cada iteracion!\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Creamos los loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,multiprocessing_context='spawn'\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2,multiprocessing_context='spawn'\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2,multiprocessing_context='spawn'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-MWu-91XzzT"
      },
      "source": [
        "### Modelo Convolucional\n",
        "\n",
        "Igual que con el modelo FeedForward, para crear un modelo usando convoluciones necesitamos crear una clase, definir los metodos **init** y **forward** y especificar la arcquitectura y comportamiento de los componentes del modelo.\n",
        "\n",
        "En particular vamos a usar:\n",
        "\n",
        "- capas convolucionales de 2D (https://pytorch.org/docs/stable/nn.html#conv2d) a las que tenemos que especificarles la cantidad de canales de entrada (1 para gris, 3 para color y X para el resultado de un filtro anterior), una cantidad de filtros a usar (out_channels), el tamaño de los mismos (kernel_size) y si aplicamos padding (relleno) o no (esto nos permite hacer convoluciones que no modifiquen el tamaño original de las imagenes).\n",
        "\n",
        "- Capas de maxpooling (https://pytorch.org/docs/stable/nn.html#maxpool2d) a las que tenemos que decirles el tamaño de la ventana a mirar y el largo del paso que deben tomar (stride).\n",
        "\n",
        "- Finalmente tambien haremos uso de capas lineales y ReLUs como hicimos anteriormente.\n",
        "\n",
        "## Modelo\n",
        "\n",
        "Vamos a Implementar LeNet (cambiando tanh por ReLU) para comparar con el modelo lineal de la clase pasada.\n",
        "\n",
        "***\n",
        "\n",
        "LeNet:\n",
        "\n",
        "\n",
        "\n",
        "![Image](https://media5.datahacker.rs/2018/11/leNet5_2.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fMBF3eElXzzT"
      },
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self, in_channels, number_classes):\n",
        "    super(LeNet,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "    self.linear1 = nn.Linear(16*5*5, 120) # 400 in_features, 120 out_features\n",
        "    self.linear2 = nn.Linear(120, 84) # 120 in_features, 84 out_features\n",
        "    self.linear3 = nn.Linear(84, number_classes) # 84 in_features, 10 out_features\n",
        "\n",
        "  def forward(self,x):\n",
        "    c1 = F.relu(self.conv1(x))\n",
        "    p1 = nn.MaxPool2d(2,2)(c1)\n",
        "    c2 = F.relu(self.conv2(p1))\n",
        "    p2 = nn.MaxPool2d(2,2)(c2)\n",
        "    flattened = torch.flatten(p2,1)\n",
        "    l1 = self.linear1(flattened)\n",
        "    l2 = self.linear2(l1)\n",
        "    out = F.relu(self.linear3(l2))\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JjiOZHcA5Lh8"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, loss_func, optimizer, epochs):\n",
        "    for epoch in range(epochs):  # Iteramos sobre el dataset entero muchas veces\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(train_loader):\n",
        "            # Nuestros datos son imagenes y la clase de cada una.\n",
        "            images, labels = data\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Reseteamos los gradientes de los pesos del modelo.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Obtenemos las predicciones para las nuevas imagenes llamando a nuestro modelo.\n",
        "            predictions = model(images)\n",
        "\n",
        "            # Calulamos el costo de nuestras predicciones respecto a la verdad\n",
        "            loss = loss_func(predictions, labels)\n",
        "\n",
        "            # Computamos los gradientes con backward y actualizamos los pesos con un optimizer.step()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Estadísiticas\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Training Epoch: {epoch + 1}, - Loss: {running_loss / len(train_loader):.5f}\")\n",
        "\n",
        "        test_epoch(model, val_loader, loss_func, \"Validation\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EFq9ebAy7GTh"
      },
      "outputs": [],
      "source": [
        "def test_epoch(model, test_loader, loss_func, name):\n",
        "  # print(name,\"Epoch: \")\n",
        "  running_loss = 0.0\n",
        "  running_error = 0.0\n",
        "\n",
        "  for i, data in enumerate(test_loader):\n",
        "    images, labels = data\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "      predictions = model(images)\n",
        "      loss = loss_func(predictions, labels)\n",
        "      running_loss += loss.item()\n",
        "      running_error += torch.sum(torch.argmax(predictions,axis=1) != labels)\n",
        "  print(f\"{name} Epoch: - Loss: {running_loss / len(test_loader):.5f}  Accuracy: {1 - running_error / len(test_loader.dataset):.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrGdIOrgqYn3",
        "outputId": "7064e81e-5fdb-43d0-bea7-030cdb31103d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch: 1, - Loss: 0.71362\n",
            "Validation Epoch: - Loss: 0.45118  Accuracy: 0.83750\n",
            "Training Epoch: 2, - Loss: 0.41593\n",
            "Validation Epoch: - Loss: 0.43006  Accuracy: 0.84883\n",
            "Training Epoch: 3, - Loss: 0.38250\n",
            "Validation Epoch: - Loss: 0.37965  Accuracy: 0.86142\n",
            "Training Epoch: 4, - Loss: 0.36173\n",
            "Validation Epoch: - Loss: 0.36413  Accuracy: 0.87050\n",
            "Training Epoch: 5, - Loss: 0.34778\n",
            "Validation Epoch: - Loss: 0.38171  Accuracy: 0.86008\n",
            "Training Epoch: 6, - Loss: 0.32745\n",
            "Validation Epoch: - Loss: 0.33975  Accuracy: 0.87492\n",
            "Training Epoch: 7, - Loss: 0.32014\n",
            "Validation Epoch: - Loss: 0.36848  Accuracy: 0.86533\n",
            "Training Epoch: 8, - Loss: 0.31757\n",
            "Validation Epoch: - Loss: 0.33525  Accuracy: 0.87883\n",
            "Training Epoch: 9, - Loss: 0.31233\n",
            "Validation Epoch: - Loss: 0.33152  Accuracy: 0.88058\n",
            "Training Epoch: 10, - Loss: 0.30136\n",
            "Validation Epoch: - Loss: 0.31861  Accuracy: 0.88783\n"
          ]
        }
      ],
      "source": [
        "conv_model = LeNet(in_channels=1, number_classes=10).to(DEVICE)\n",
        "\n",
        "LEARNING_RATE = 0.03\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "conv_optimizer = optim.SGD(conv_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "conv_model = train_model(conv_model, train_loader, val_loader, loss_func=criterion, optimizer=conv_optimizer, epochs=10)\n",
        "# test_epoch(conv_model, test_loader, criterion, \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3uHiDViqYn6",
        "outputId": "054d725d-917d-458e-c7a7-e86fe810bec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Epoch: - Loss: 0.33824  Accuracy: 0.88080\n"
          ]
        }
      ],
      "source": [
        "test_epoch(conv_model,test_loader,criterion,\"Test\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN_01_Letra.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}