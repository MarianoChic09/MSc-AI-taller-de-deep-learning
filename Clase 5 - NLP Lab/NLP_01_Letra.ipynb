{"cells":[{"cell_type":"markdown","metadata":{"id":"7VMtkNDwxZ0m"},"source":["# Datos\n","\n","Vamos a usar el dataset de IMDB para clasificación de reseñas de películas, el objetivo del mismo es detectar si una reseña tiene sentimiento **positivo** o **negativo**.\n","\n","Descarguen el dataset de este [link](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n","\n","Y word2vec de este [link](https://drive.google.com/file/d/1XusPRjsCVcIdCQ2hQDDWcH_wayfn4nWb/view?usp=sharing).\n","\n","-> Para correr esta notebook en colab suban los archivos a una carpeta **data** en la raiz de su drive personal.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37098,"status":"ok","timestamp":1632863784197,"user":{"displayName":"Matias Sorozabal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04842478827197692237"},"user_tz":180},"id":"zh7A9mjs3cfU","outputId":"b235382f-0109-4101-849a-d91c44fbac9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"," drive\t'IMDB Dataset.csv'   sample_data   word2vec.txt\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","! cp \"/content/drive/My Drive/data/IMDB_Dataset.zip\" .\n","! unzip -q IMDB_Dataset.zip\n","! rm IMDB_Dataset.zip\n","! ls"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"BBaFc6ZZzp2T"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['review', 'sentiment'], dtype='object')\n"]}],"source":["import pandas as pd\n","imdb_data = pd.read_csv(\"IMDB Dataset.csv\")\n","\n","#sentiment count\n","print(imdb_data.columns)\n","imdb_data['sentiment'].value_counts()\n","\n","# Convert positive and negative into binary classes (1-0)\n","from sklearn.preprocessing import LabelBinarizer\n","lb = LabelBinarizer()\n","\n","sentiment_data = lb.fit_transform(imdb_data[\"sentiment\"])\n","imdb_data['sentiment'] = sentiment_data"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"rQFZbEtC1T94"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Probably my all-time favorite movie, a story o...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>I sure would like to see a resurrection of a u...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>This show was an amazing, fresh &amp; innovative i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Encouraged by the positive comments about this...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>If you like original gut wrenching laughter yo...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  sentiment\n","0  One of the other reviewers has mentioned that ...          1\n","1  A wonderful little production. <br /><br />The...          1\n","2  I thought this was a wonderful way to spend ti...          1\n","3  Basically there's a family where a little boy ...          0\n","4  Petter Mattei's \"Love in the Time of Money\" is...          1\n","5  Probably my all-time favorite movie, a story o...          1\n","6  I sure would like to see a resurrection of a u...          1\n","7  This show was an amazing, fresh & innovative i...          0\n","8  Encouraged by the positive comments about this...          0\n","9  If you like original gut wrenching laughter yo...          1"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["imdb_data.head(10)"]},{"cell_type":"markdown","metadata":{"id":"rhdlJtOo2YC-"},"source":["# Imports\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"YaxC7dfq2aKT"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\franz\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping corpora\\stopwords.zip.\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\franz\\AppData\\Roaming\\nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","from bs4 import BeautifulSoup\n","\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"markdown","metadata":{"id":"Lkr7Vz950IeY"},"source":["# Preprocesamiento Inicial\n","\n","Como toda tarea de NLP tenemos que comenzar preprocesando los datos, eliminando palabras que no nos sirve, caracteres especiales, etc.\n","\n","En particular hay tres tareas a ser realizadas basadas en un análisis inicial del dataset (mirando ejemplos al azar del mismo)\n","\n","\n","\n","1.   Eliminar tags html (vamos a utilizar BeautifulSoup para esto)\n","2.   Eliminar texto entre parentesis rectos (Usando la siguiente expresion regular: ```\\[[^]]*\\]``` )\n","3. Eliminar caracteres especiales, usando una regex quitar todos los caracteres que no son ni letras ni números (```[^a-zA-z0-9\\s] ``` )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIiaBeGOwIOQ"},"outputs":[],"source":["def strip_html(text):\n","  pass\n","\n","def remove_between_square_brackets(text):\n","  pass\n","\n","def remove_special_characters(text):\n","  pass\n","\n","def low_level_preproc(text):\n","  pass\n","\n","#Apply function on review column\n","imdb_data['review'] = imdb_data['review'].apply(low_level_preproc)"]},{"cell_type":"markdown","metadata":{"id":"gGJUuc0O16iv"},"source":["# Preprocesamiento de alto nivel\n","\n","Una vez tenemos el texto limpio y trabajable volvemos a hacer otro pasaje de preprocesamiento de más alto nivel, ahora vamos a querer:\n","\n","\n","\n","1.   Transformar todo el texto a minúscula\n","2.   Quitar stop words (usando nltk)\n","3.   Lemmatizar usando nltk WordNetLemmatizer\n","\n","Para todo esto vamos a necesitar trabajar con **tokens** palabras individuales, en este caso vamos a separar por **whitespace**, pero se podrían usar mejores estrategias.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KeKNTxvzwISR"},"outputs":[],"source":["all_stopwords = set(stopwords.words(\"english\"))\n","\n","def remove_stop_words(full_text_line):\n","  pass\n","\n","def lemmatize(text):\n","  pass\n","\n","def high_level_preproc(text):\n","  pass\n","\n","#Apply function on review column\n","imdb_data['review'] = imdb_data['review'].str.lower()\n","imdb_data['review'] = imdb_data['review'].apply(high_level_preproc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9k3wz_XUql1d"},"outputs":[],"source":["imdb_data['review'].head(10)"]},{"cell_type":"markdown","metadata":{"id":"MAynpGKc3dYn"},"source":["# Modelando\n","\n","Para modelar vamos a comenzar separando el dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFZn34Q63lLE"},"outputs":[],"source":["#split the dataset  \n","#train dataset\n","train_reviews = imdb_data.review[:40000]\n","train_sentiments = imdb_data.sentiment[:40000]\n","\n","#test dataset\n","test_reviews = imdb_data.review[40000:]\n","test_sentiments = imdb_data.sentiment[40000:]\n","\n","\n","print(\"Train set:\", train_reviews.shape, train_sentiments.shape)\n","print(\"Test set:\", test_reviews.shape, test_sentiments.shape)"]},{"cell_type":"markdown","metadata":{"id":"U16zBbGzAb5z"},"source":["Vamos a generar vectores para las reseñas usando TF-IDF (sklearn). Vamos a hacer uso del parametro ```max_features``` que nos permite controlar cuántas palabras considerar para generar los vectores (en orden de frecuencia). Luego usamos esa representacion vectorial para entrenar y testear un regresor logístico (LogisticRegressor). En particular vamos a empezar con 300 features, más adelante veremos por qué.\n","\n","\n","Vamos a entrenar el modelo por 500 iteraciones como máximo y usamos l2 como regularizador."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RVouRZ_ql3I"},"outputs":[],"source":["# Su código para vectorizar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCOsCtrEAaJj"},"outputs":[],"source":["# Su código para el modelo"]},{"cell_type":"markdown","metadata":{"id":"btnSQOb2JwIL"},"source":["# Vectores pre entrenados\n","Ahora vamos a ver si podemos superar la performance del modelo haciendo uso de deep learning.  Primero vamos a entrenar el mismo modelo usando los embeddings preentrenados de Word2Vec (usando gensim). \n","\n","Luego vamos a darle esos embeddings a un MLP y ver si logramos superar la performance anterior."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYqJ0uJ1XH6Y"},"outputs":[],"source":["import gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7oiP209Jvs1"},"outputs":[],"source":["w2v = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec.txt\", binary=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ap22Pw93U586"},"outputs":[],"source":["mean_vector = np.mean(w2v.vectors, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVwCEMusJvw0"},"outputs":[],"source":["def get_sentence_embedding(text):\n","  pass\n","\n","\n","train_vectors = [get_sentence_embedding(sent) for sent in train_reviews]\n","test_vectors = [get_sentence_embedding(sent) for sent in test_reviews]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"884gkcqLVvyL"},"outputs":[],"source":["# Su código para el modelo (Igual que antes)"]},{"cell_type":"markdown","metadata":{"id":"-2GvqL87Yipe"},"source":["# Deep Learning\n"]},{"cell_type":"markdown","metadata":{"id":"5oxjMLJAW8qr"},"source":["MLP: vamos a crear un MLP para atacar ese mismo problema, el diseño corre por su cuenta pero deberían ser capaces de obetener mejor performance en test que los modelos anteriores.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqH91xLsYlQr"},"outputs":[],"source":["# Imports\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(DEVICE)\n","\n","torch.manual_seed(42)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUuaAbANZscl"},"outputs":[],"source":["def train_epoch(training_model, loader, criterion, optim):\n","    training_model.train()\n","    epoch_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    for data, labels in loader:\n","      all_labels.extend(labels.numpy())  \n","\n","      optim.zero_grad()\n","\n","      predictions = training_model(data.to(DEVICE))\n","      all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","\n","      loss = criterion(predictions, labels.to(DEVICE))\n","      \n","      loss.backward()\n","      optim.step()\n","\n","      epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n","\n","\n","def validation_epoch(val_model, loader, criterion):\n","    val_model.eval()\n","    epoch_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    with torch.no_grad():\n","      for data, labels in loader:\n","        all_labels.extend(labels.numpy())  \n","\n","        predictions = val_model(data.to(DEVICE))\n","        all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","\n","        loss = criterion(predictions, labels.to(DEVICE))\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n","  \n","\n","def train_model(model, train_loader, test_loader, criterion, optim, number_epochs):\n","  train_history = []\n","  test_history = []\n","  accuracy_history = []\n","\n","  for epoch in range(number_epochs):\n","      start_time = time.time()\n","\n","      train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n","      train_history.append(train_loss)\n","      print(\"Training epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n","            .format(epoch + 1, train_loss, train_acc, time.time() - start_time))\n","\n","      start_time = time.time()\n","      test_loss, acc = validation_epoch(model, test_loader, criterion)\n","      test_history.append(test_loss)\n","      accuracy_history.append(acc)\n","      print(\"Validation epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n","            .format(epoch + 1, test_loss, acc, time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M46rdl18JvzG"},"outputs":[],"source":["class MLP(nn.Module):\n","\n","  def __init__(self, in_features):\n","    super(MLP, self).__init__()\n","    # Su implementacion\n","\n","  def forward(self, new_input):\n","    # Su implementacion\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wl3PabnAX4_1"},"outputs":[],"source":["loss_function = nn.CrossEntropyLoss().to(DEVICE)\n","modelo = MLP(in_features=300).to(DEVICE)\n","optimizer = torch.optim.Adam(modelo.parameters(), lr=0.001)\n","BATCH_SIZE = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCauiKoIX5CJ"},"outputs":[],"source":["# Dataloaders\n","train_vectors = [get_sentence_embedding(sent) for sent in train_reviews]\n","test_vectors = [get_sentence_embedding(sent) for sent in test_reviews]\n","\n","train_targets = torch.Tensor(train_sentiments.to_numpy()).long()\n","train_dataset = TensorDataset(torch.Tensor(train_vectors), train_targets) \n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=2)\n","\n","test_targets = torch.Tensor(test_sentiments.to_numpy()).long()\n","test_dataset = TensorDataset(torch.Tensor(test_vectors), test_targets) \n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDqPZnJgZWDi"},"outputs":[],"source":["train_model(modelo, train_dataloader, test_dataloader, loss_function, optimizer, 10)"]},{"cell_type":"markdown","metadata":{"id":"zBfOKG3iMt9Q"},"source":["# Exploración\n","\n","Exploren otras técnicas de preprocesamiento, tokenizacion, vectorizacion, etc. para ver si puede lograr superar los modelos presentados en clase.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T09IUvPVJv06"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["zBfOKG3iMt9Q"],"name":"NLP_01_Letra.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}
