{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNs_2_Letra.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kM-aVQsV0Ie"
      },
      "source": [
        "# Imports globales y funciones de utilidad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpqMPo4nV4UB"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import itertools\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGyynDexV60_"
      },
      "source": [
        "def get_dataloaders(train_transf, test_transf, batch_size):\n",
        "\n",
        "  train_dataset = CIFAR10(\"data\", train=True, download=True, transform=train_transf)\n",
        "  test_dataset = CIFAR10(\"data\", train=False, download=True, transform=test_transf)\n",
        "\n",
        "  train_size = int(0.8 * len(train_dataset))\n",
        "  valid_size = len(train_dataset) - train_size\n",
        "  train_dataset, validation_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
        "  valid_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
        "\n",
        "  return train_loader, valid_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hZyH1P8Velv"
      },
      "source": [
        "# Image Augmentation\n",
        "\n",
        "La primera parte de este laboratorio consiste en expandir un poco el modelo de LeNet para tener más parámetros y luego explorar algunas técnicas de Image Augmentation (https://pytorch.org/vision/stable/transforms.html).\n",
        "\n",
        "El modelo a implementar es el siguiente:\n",
        "\n",
        "\n",
        "![Image](https://i.ibb.co/WxGgbmL/Capture.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chgl8-GKTyfV"
      },
      "source": [
        "class CustomCNN(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    # in_channels: int, cantidad de canales de la imagen original\n",
        "    super(CustomCNN, self).__init__()\n",
        "    # Su implementacion\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Su implementacion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGisRfZ8bYXW"
      },
      "source": [
        "## Funciones genericas para entrenar nuestros modelos\n",
        "\n",
        "Vamos a utilizar las mismas funciones que implementamos en los laboratorios anteriores para entrenar y testear nuestros modelos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Gp496cWTyh-"
      },
      "source": [
        "def train_epoch(training_model, loader, criterion, optim):\n",
        "    training_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    \n",
        "    for images, labels in loader:\n",
        "      all_labels.extend(labels.numpy())  \n",
        "\n",
        "      optim.zero_grad()\n",
        "\n",
        "      predictions = training_model(images.to(device))\n",
        "      all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n",
        "\n",
        "      loss = criterion(predictions, labels.to(device))\n",
        "      \n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n",
        "\n",
        "\n",
        "def validation_epoch(val_model, loader, criterion):\n",
        "    val_model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for images, labels in loader:\n",
        "        all_labels.extend(labels.numpy())  \n",
        "\n",
        "        predictions = val_model(images.to(device))\n",
        "        all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n",
        "\n",
        "        loss = criterion(predictions, labels.to(device))\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n",
        "  \n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optim, number_epochs):\n",
        "  train_history = []\n",
        "  test_history = []\n",
        "  accuracy_history = []\n",
        "\n",
        "  for epoch in range(number_epochs):\n",
        "      start_time = time.time()\n",
        "\n",
        "      train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "      train_history.append(train_loss)\n",
        "      print(\"Training epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n",
        "            .format(epoch + 1, train_loss, train_acc, time.time() - start_time))\n",
        "\n",
        "      start_time = time.time()\n",
        "      test_loss, acc = validation_epoch(model, test_loader, criterion)\n",
        "      test_history.append(test_loss)\n",
        "      accuracy_history.append(acc)\n",
        "      print(\"Validation epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n",
        "            .format(epoch + 1, test_loss, acc, time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrGeplg5bmKm"
      },
      "source": [
        "## Entrenando modelos \n",
        "\n",
        "Comenzamos definiendo una seccion de código con valores por defecto de hiperparámetros que vamos a utilizar y luego entrenamos un modelo de la CNN definida anteriormente sin usar augmentation en los datos y otro haciendo uso del mismo.\n",
        "\n",
        "https://pytorch.org/vision/stable/transforms.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj_T5Fza6efw"
      },
      "source": [
        "# Global models config\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LR = 0.001\n",
        "NUMBER_EPOCHS = 15\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "137kQNukTyoI"
      },
      "source": [
        "# Fijamos las semillas siempre para poder comparar.\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Creamos los dataloaders\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_loader, valid_loader, test_loader = get_dataloaders(train_transform, test_transform, BATCH_SIZE)\n",
        "\n",
        "# Definimos el modelo y el optimizador\n",
        "modelo_sin_aug = CustomCNN(3).to(device)\n",
        "optimizer = torch.optim.Adam(modelo_sin_aug.parameters(), lr=LR)\n",
        "\n",
        "# Entrenamos\n",
        "train_model(modelo_sin_aug, train_loader, valid_loader, criterion, optimizer, NUMBER_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_c-x46dYUyk"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Creamos los datasets\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Definir transormaciones que vamos a aplicar al set de entrenamiento\n",
        "train_transform = transforms.Compose([\n",
        "    # ?? transforms.??? \n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_loader, valid_loader, test_loader = get_dataloaders(train_transform, test_transform, BATCH_SIZE)\n",
        "\n",
        "\n",
        "# Crear el modelo, optimizador y entrenarlo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pduauAlXcqZt"
      },
      "source": [
        "## Evaluando los modelos en los datos de test\n",
        "\n",
        "Vamos a comenzar evaluando en los datos de test normales y luego vamos a aplicar distintas transformaciones (para simular entornos más reales de datos) y vamos a ver la performance y robustez de los modelos que entrenamos anteriormente.\n",
        "\n",
        "Primero agregar horizontal flip y luego vertical flip, qué pasa con los modelos?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNp0-FmEFH16"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "  transforms.ToTensor()                              \n",
        "])\n",
        "\n",
        "_, _, test_loader = get_dataloaders(None, test_transform, BATCH_SIZE)\n",
        "\n",
        "# Testear usando las funciones definidas anteriormente"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8XEXFoo8jfG"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "  # Flip Horizontal y volver a testear\n",
        "  transforms.ToTensor()                              \n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3DMvkFf9EQg"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "  # Flip Vertical y volver a testear\n",
        "  transforms.ToTensor()                              \n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OPa2WNqgXyy"
      },
      "source": [
        "# Otros tests que quieran probar..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsQVBuQKwOvX"
      },
      "source": [
        "# DenseNet\n",
        "\n",
        "\n",
        "![Image](https://miro.medium.com/max/5164/1*_Y7-f9GpV7F93siM1js0cg.jpeg)\n",
        "\n",
        "Link al paper original: [DenseNets](https://arxiv.org/pdf/1608.06993.pdf)\n",
        "\n",
        "Algunas consideraciones del paper a tener en cuenta:\n",
        "\n",
        "1. Batch normalization en los inputs de los bloques densos y las capas de transición.\n",
        "2. ReLU en todos lados como funcion de activación.\n",
        "3. El MLP al final de la red cuenta con una capa oculta de 512 neuronas\n",
        "4. Las activaciones luego del tercer bloque denso tienen tamaño 4*4 (ejercicio, calcular a mano!)\n",
        "\n",
        "\n",
        "Implementamos DenseNet para resolver el problema de CIFAR10\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeyVzE9bLUTA"
      },
      "source": [
        "class DenseBlock(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super(DenseBlock, self).__init__()\n",
        "    # Su implementacion\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # Su implementacion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTjRTakNLVGm"
      },
      "source": [
        "class TransitionLayer(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(TransitionLayer, self).__init__()\n",
        "    # Su implementacion\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Su implementacion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0rglwp_wWEF"
      },
      "source": [
        "class DenseNet(nn.Module):\n",
        "\tdef __init__(self, n_classes):\n",
        "\t\tsuper(DenseNet, self).__init__()\n",
        "    # Su implementacion\n",
        "\n",
        "\tdef forward(self, x):\n",
        "    # Su implementacion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7vhkbNiwWIU"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Creamos los datasets\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "densenet = DenseNet(10).to(device)\n",
        "optimizer = torch.optim.Adam(densenet.parameters(), lr=LR)\n",
        "\n",
        "train_loader, valid_loader, test_loader = get_dataloaders(train_transform, test_transform, BATCH_SIZE)\n",
        "\n",
        "train_model(densenet, train_loader, valid_loader, criterion, optimizer, NUMBER_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G9PbgeDdD_s"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "  transforms.ToTensor()                              \n",
        "])\n",
        "\n",
        "_, _, test_loader = get_dataloaders(None, test_transform, BATCH_SIZE)\n",
        "\n",
        "test_loss, accuracy = validation_epoch(densenet, test_loader, criterion)\n",
        "print(f\"DenseNet Test set: {test_loss:.6f} Loss. Accuracy {accuracy:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}