{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwmbSlbbFuM5"
      },
      "source": [
        "# Traduccion usando modelos Seq2Seq\n",
        "\n",
        "Este notebook está fuertemente basado en el tutorial de PyTorch [*NLP From Scratch: Translation with a Sequence to Sequence Network and Attention*](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) creado por Sean Robertson.\n",
        "\n",
        "Vamos a estar viendo como traducir frases en francés al inglés.\n",
        "\n",
        "    IN: il est en train de peindre un tableau .\n",
        "    TRG: he is painting a picture .\n",
        "    OUT: he is painting a picture .\n",
        "\n",
        "    IN: pourquoi ne pas essayer ce vin delicieux ?\n",
        "    TRG: why not try that delicious wine ?\n",
        "    OUT: why not try that delicious wine ?\n",
        "\n",
        "    IN: elle n est pas poete mais romanciere .\n",
        "    TRG: she is not a poet but a novelist .\n",
        "    OUT: she not not a poet but a novelist .\n",
        "\n",
        "    IN: vous etes trop maigre .\n",
        "    TRG: you re too skinny .\n",
        "    OUT: you re all alone .\n",
        "\n",
        "... con distintos niveles de éxito.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypwInf2vMvW4"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v2KFieVrFuM6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1gKbmYbFuNG"
      },
      "source": [
        "Preparando los Datos\n",
        "==================\n",
        "\n",
        "Los datos para este problema son miles de parejas de sentencias en inglés y francés.\n",
        "\n",
        "    I am cold.    J'ai froid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "74vkAp2HOJ5G"
      },
      "outputs": [],
      "source": [
        "# !wget https://download.pytorch.org/tutorial/data.zip\n",
        "# !unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Maf203LiOjQz"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>French</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fire!</td>\n",
              "      <td>Au feu !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135837</th>\n",
              "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
              "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135838</th>\n",
              "      <td>Death is something that we're often discourage...</td>\n",
              "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135839</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135840</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135841</th>\n",
              "      <td>It may be impossible to get a completely error...</td>\n",
              "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135842 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  English  \\\n",
              "0                                                     Go.   \n",
              "1                                                    Run!   \n",
              "2                                                    Run!   \n",
              "3                                                    Wow!   \n",
              "4                                                   Fire!   \n",
              "...                                                   ...   \n",
              "135837  A carbon footprint is the amount of carbon dio...   \n",
              "135838  Death is something that we're often discourage...   \n",
              "135839  Since there are usually multiple websites on a...   \n",
              "135840  If someone who doesn't know your background sa...   \n",
              "135841  It may be impossible to get a completely error...   \n",
              "\n",
              "                                                   French  \n",
              "0                                                    Va !  \n",
              "1                                                 Cours !  \n",
              "2                                                Courez !  \n",
              "3                                              Ça alors !  \n",
              "4                                                Au feu !  \n",
              "...                                                   ...  \n",
              "135837  Une empreinte carbone est la somme de pollutio...  \n",
              "135838  La mort est une chose qu'on nous décourage sou...  \n",
              "135839  Puisqu'il y a de multiples sites web sur chaqu...  \n",
              "135840  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
              "135841  Il est peut-être impossible d'obtenir un Corpu...  \n",
              "\n",
              "[135842 rows x 2 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Take a peek at the dataset\n",
        "dataset = pd.read_csv(\"data/eng-fra.txt\", sep=\"\\t\", header=None)\n",
        "dataset.columns = [\"English\", \"French\"]\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKz4kryVFuNH"
      },
      "source": [
        "We will represent each word in a language as a one-hot vector (i.e., a giant vector of zeros except for a single one at the index of the word). We will however cheat a bit and trim the data to only use a few thousand words per language.\n",
        "\n",
        "Vamos a representar cada palabra como un one-hot encoded vector (un índice por palabra). Para esto vamos a crear un vocabulario y limitar el número máximo de palabras para solamente usar unas cuántas miles de palabras por lenguaje. \n",
        "\n",
        "\n",
        "![Word encoding](https://drive.google.com/uc?id=1aLm__m9YWaKRZdDmdInE5rT-et0jXTci \"Word encoding\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdE1cMrYFuNJ"
      },
      "source": [
        "Vamos a necesitar un índice por palabra, para esto (y como hemos hecho antes) vamos a crear un vocabulario. En particular en este caso vamos a  hacer uso de una clase auxiliar `Lang` que tiene:\n",
        "  - word → index (``word2index``) \n",
        "  - index → word (``index2word``)\n",
        "  - ``word2count`` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WKLy6SSnFuNL"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "      for word in sentence.split(\" \"):\n",
        "        self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "      if word not in self.word2index:\n",
        "        self.word2index[word] = self.n_words\n",
        "        self.word2count[word] = 1\n",
        "        self.index2word[self.n_words] = word\n",
        "        self.n_words += 1\n",
        "      else:\n",
        "        self.word2count[word] += 1\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJxMTG6xFuNR"
      },
      "source": [
        "Los archivos estan en Unicode, para simplificar los transformamos a ASCII, pasamos todo a minúscula y quitamos la mayor parte de la puntuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HtfSmhwGFuNS"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicode2ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode2ascii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fLdxTL5FuNZ"
      },
      "source": [
        "Para leer los datos necesitamos leer cada línea a la vez, y luego separar cada línea en las dos sentencias que la componen. Todos los archivos que descargamos están en Inglés → Otro Idioma, por lo que si queremos traducir desde Otro Idioma → Inglés tenemos que usar la flag `reverse` para invertir los pares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "22jUHxw2FuNa"
      },
      "outputs": [],
      "source": [
        "def read_langs(lang1, lang2, reverse=False):\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-PS-XSMFuNh"
      },
      "source": [
        "Como tenemos muchos ejemplos y queremos entrenar algo rápidamente, vamos a recortar los datos a un máximo de 10 palabras y nos quedamos con sentencias que se traducen a la forma \"I am\", \"He is\", etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xpNtFi7mFuNh"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filter_pair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filter_pairs(pairs):\n",
        "    return [pair for pair in pairs if filter_pair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rymw2EPmFuNm"
      },
      "source": [
        "El proceso completo para preparar los datos es:\n",
        "\n",
        "- Leer el archivo, separarlo en líneas y separar cada línea en parejas\n",
        "- Normalizar y filtrar los textos\n",
        "- Crear los vocabularios a partir de los pares\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nkFbqs75FuNn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "['je suis heureuse de vous avoir invitee .', 'i m glad i invited you .']\n"
          ]
        }
      ],
      "source": [
        "def prepare_data(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    pairs = filter_pairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    for pair in pairs:\n",
        "      input_lang.add_sentence(pair[0])\n",
        "      output_lang.add_sentence(pair[1])\n",
        "\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    \n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Au3B9qFuNs"
      },
      "source": [
        "Modelo\n",
        "=================\n",
        "\n",
        "![Seq2Seq Architecture](https://drive.google.com/uc?id=14XIFBXqpos7Z_spBMtK5gyWbl5MJyMud \"Seq2Seq Architecture\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0UeYlSqFuNs"
      },
      "source": [
        "Encoder\n",
        "-----------\n",
        "\n",
        "![Encoder Network](https://drive.google.com/uc?id=17D4YBVh630jJBo6TVquS2a1R6XmPs3qK \"Encoder Network\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "PT6ON9nzFuNt"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size,embedding_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "\n",
        "    def forward(self, input_data, hidden):\n",
        "      # los embeddings son una palabra a la vez, va a ser necesario hacer un .view(1, 1, -1)\n",
        "      embedded = self.embedding(input_data).view(1, 1, -1)\n",
        "      output, hidden = self.gru(embedded, hidden)\n",
        "      return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nti9yrscFuNz"
      },
      "source": [
        "## Decoder Simple\n",
        "\n",
        "![Decoder Network](https://drive.google.com/uc?id=13kddnNWcPFku6SUS4ZbTMnDZLmyB2baD \"Decoder Network\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CHb4ek-pFuN0"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, embedding_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        _, hidden = self.gru(output, hidden)\n",
        "        output = self.out(hidden)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmT58gbVU6JN"
      },
      "source": [
        "# Funciones Auxiliares\n",
        "\n",
        "### Preparando los datos\n",
        "\n",
        "Vamos a transformar cada pareja de sentencias a una tupla de tensores con índices. Al crearlos, vamos a agregar el token de EOS en ambos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BVGLn2CEFuOB"
      },
      "outputs": [],
      "source": [
        "def indexes_from_sentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
        "\n",
        "\n",
        "def tensor_from_sentence(lang, sentence):\n",
        "    indexes = indexes_from_sentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensors_from_pair(pair):\n",
        "    input_tensor = tensor_from_sentence(input_lang, pair[0])\n",
        "    target_tensor = tensor_from_sentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc670q3tFuOE"
      },
      "source": [
        "### Entrenando el modelo\n",
        "------------------\n",
        "\n",
        "Para entrenar el modelo, pasamos la sentencia de entrada (palabra a palabra) a través del encoder y nos quedamos con sus outputs y último hidden state. El decoder luego recibe el token de `<SOS>` como primer input y el hidden state del encoder como su hidden state inicial. \n",
        "\n",
        "\"Teacher Forcing\" es el concepto de usar el target real como inputs nuevos para cada paso, en lugar de usar las predicciones del decoder. Esto ayuda a la convergencia pero puede traer inestabilidad si la red es explotada: [inestabilidad](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n",
        "\n",
        "Gracias a la libertad que nos da autograd de PyTorch podemos elegir usar teacher forcing solamente un porcentaje de las veces con un simple if, y nuestros optimizadores funcionan sin alterarse. En particular vamos a usar `teacher_forcing_ratio` de 0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "FrDllDWwFuOF"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Reset optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Feed inputs to the encoder one by one ~3 Lines\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "    for i in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "\n",
        "    # Initialize decoder input and hidden state ~2 Lines\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    use_teacher_forcing =  random.random() < teacher_forcing_ratio  # Randomly choose whether to use teacher forcing. \n",
        "    #Teacher forcing is useful when the target output is known beforehand, but it may lead to suboptimal performance during inference time (when we don't know the target output beforehand). \n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        # Feed the decoder inputs one by one, add the loss at each timestep and move forward using the next target as input\n",
        "        # ~3 Lines\n",
        "        for j in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output.squeeze(1), target_tensor[j])\n",
        "            decoder_input = target_tensor[j]\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        # Feed the decoder inputs one by one, find the predicted next token and set it as next input (use .detach() on this tensor)\n",
        "        # Compute the loss at each timestep\n",
        "        # If the decoder generates an EOS, stop.\n",
        "        # ~8 Lines\n",
        "        for j in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output.squeeze(1), target_tensor[j])\n",
        "            \n",
        "            decoder_input = torch.argmax(decoder_output).detach()\n",
        "            if decoder_input == EOS_token:\n",
        "                break\n",
        "    # Backprop! ~3 Lines\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r29fiUAkFuOK"
      },
      "source": [
        "Funciones auxiliares para contabilizar el tiempo y estimar el tiempo restante.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Th1RxXtXFuOL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def as_minutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def time_since(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeaDGFCwFuOP"
      },
      "source": [
        "Todo el proceso de entrenamiento consiste en:\n",
        "\n",
        "-  Comenzar un timer\n",
        "-  Inicializar los optimizadores y el costo. Vamos a usar NLLLoss como costo.\n",
        "-  Crear un set de parejas de entrenamiento\n",
        "-  Inicializar array vacío para los costos\n",
        "\n",
        "Luego llamamos a ``train`` muchas veces y ocasionalmente imprimimos el progreso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "kAF-qTysFuOP"
      },
      "outputs": [],
      "source": [
        "def train_iters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [\n",
        "        tensors_from_pair(random.choice(pairs))\n",
        "        for i in range(n_iters)\n",
        "    ]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(\n",
        "            input_tensor, target_tensor, encoder, decoder,\n",
        "            encoder_optimizer, decoder_optimizer, criterion\n",
        "        )\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (\n",
        "                time_since(start, iter / n_iters),\n",
        "                iter,\n",
        "                iter / n_iters * 100,\n",
        "                print_loss_avg\n",
        "            ))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    show_plot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qT5cU5GFuOS"
      },
      "source": [
        "Mostrando los resultados\n",
        "----------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "yps1cwT1FuOU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def show_plot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLmCrf3YFuOY"
      },
      "source": [
        "Evaluación\n",
        "==========\n",
        "\n",
        "La evaluacion se hace de igual manera que el entrenamiento, pero, al no tener objetivos, usamos las mismas predicciones del decoder como inputs. Hacemos esto hasta que el decorer genere un token de EOS. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "JvMJB8hLFuOZ"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sequence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        # Code to run an evaluation step, without targets for the decoder\n",
        "        # ~21 Lines\n",
        "            # Reset optimizers\n",
        "        input_tensor = tensor_from_sentence(input_lang, sequence)\n",
        "        input_length = input_tensor.size(0)\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "\n",
        "        # Feed inputs to the encoder one by one ~3 Lines\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "        for i in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "\n",
        "        # Initialize decoder input and hidden state ~2 Lines\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        length = 0 \n",
        "        output_indexes = []\n",
        "        \n",
        "        while decoder_input != EOS_token and length < max_length:\n",
        "\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)            \n",
        "            decoder_input = torch.argmax(decoder_output)\n",
        "            length += 1\n",
        "            output_indexes.append(decoder_input)\n",
        "            if decoder_input == EOS_token:\n",
        "                break\n",
        "        \n",
        "        decoded_words = [output_lang.index2word[index] for index in output_indexes]\n",
        "        \n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "pqtl4e09FuOf"
      },
      "outputs": [],
      "source": [
        "def evaluate_randomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('IN:', pair[0])\n",
        "        print('TRG:', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('OUT:', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRJd5QJuFuOi"
      },
      "source": [
        "Entrenando y Evaluando\n",
        "=======================\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "SuMbwnYiFuOi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0m 22s (- 143m 3s) (200 0%) 0.0000\n",
            "0m 31s (- 97m 51s) (400 0%) 0.0000\n",
            "0m 37s (- 78m 0s) (600 0%) 0.0000\n",
            "0m 46s (- 72m 25s) (800 1%) 0.0000\n",
            "0m 53s (- 66m 4s) (1000 1%) 0.0000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mk:\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Deep Learning\\MSc-AI-taller-de-deep-learning\\Clase 9 - Lab09 Seq2Seq\\Lab09_Seq2Seq.ipynb Cell 37\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m encoder \u001b[39m=\u001b[39m EncoderRNN(input_lang\u001b[39m.\u001b[39mn_words, hidden_size,embedding_size)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m decoder \u001b[39m=\u001b[39m DecoderRNN(hidden_size, output_lang\u001b[39m.\u001b[39mn_words,embedding_size\u001b[39m=\u001b[39membedding_size)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_iters(encoder, decoder, \u001b[39m75000\u001b[39;49m, print_every\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m)\n",
            "\u001b[1;32mk:\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Deep Learning\\MSc-AI-taller-de-deep-learning\\Clase 9 - Lab09 Seq2Seq\\Lab09_Seq2Seq.ipynb Cell 37\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m input_tensor \u001b[39m=\u001b[39m training_pair[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m target_tensor \u001b[39m=\u001b[39m training_pair[\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m train(\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     input_tensor, target_tensor, encoder, decoder,\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     encoder_optimizer, decoder_optimizer, criterion\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m plot_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
            "\u001b[1;32mk:\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Deep Learning\\MSc-AI-taller-de-deep-learning\\Clase 9 - Lab09 Seq2Seq\\Lab09_Seq2Seq.ipynb Cell 37\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# Backprop! ~3 Lines\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m encoder_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/k%3A/OneDrive/Master%20en%20Big%20Data%20-%20Universidad%20ORT/3er%20Semestre/Deep%20Learning/MSc-AI-taller-de-deep-learning/Clase%209%20-%20Lab09%20Seq2Seq/Lab09_Seq2Seq.ipynb#X51sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m decoder_optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\envs\\mario_env_windows\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\envs\\mario_env_windows\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "embedding_size = 10\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size,embedding_size).to(device)\n",
        "decoder = DecoderRNN(hidden_size, output_lang.n_words,embedding_size=embedding_size).to(device)\n",
        "\n",
        "train_iters(encoder, decoder, 75000, print_every=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Falta corregir mirar el video de la clase (algo menor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxfrex2nFuOm"
      },
      "outputs": [],
      "source": [
        "evaluate_randomly(encoder, decoder)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Lab09_Seq2Seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
