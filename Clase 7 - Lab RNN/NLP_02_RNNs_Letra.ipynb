{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_02_RNNs_Letra.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7VMtkNDwxZ0m"},"source":["# Datos y Preprocesamiento\n","\n","Vamos a usar el dataset de IMDB para clasificación de reseñas de películas, el objetivo del mismo es detectar si una reseña tiene sentimiento **positivo** o **negativo**.\n","\n","Descarguen el dataset de este [link](https://drive.google.com/file/d/1i0bBI4p80AxsLgnWcXkxVT65AahIzePu/view?usp=sharing) y subanlo a una carpeta **data** en la raiz de su drive personal.\n"]},{"cell_type":"code","metadata":{"id":"zh7A9mjs3cfU"},"source":["import pandas as pd\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","! cp \"/content/drive/My Drive/data/IMDB_Dataset.zip\" .\n","! unzip -q IMDB_Dataset.zip\n","! rm IMDB_Dataset.zip\n","! ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJYwO9zbvCLh"},"source":["import re\n","import time\n","from itertools import chain\n","from bs4 import BeautifulSoup\n","from collections import Counter\n","\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(DEVICE)\n","\n","torch.manual_seed(42)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBaFc6ZZzp2T"},"source":["imdb_data = pd.read_csv(\"IMDB Dataset.csv\")\n","\n","#sentiment count\n","print(imdb_data.columns)\n","imdb_data['sentiment'].value_counts()\n","\n","# Convert positive and negative into binary classes (1-0)\n","from sklearn.preprocessing import LabelBinarizer\n","lb = LabelBinarizer()\n","\n","sentiment_data = lb.fit_transform(imdb_data[\"sentiment\"])\n","imdb_data['sentiment'] = sentiment_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKV_HYQCvEFv"},"source":["def strip_html(text):\n","  soup = BeautifulSoup(text, \"html.parser\")\n","  return soup.get_text()\n","\n","\n","def remove_between_square_brackets(text):\n","  return re.sub('\\[[^]]*\\]', '', text)\n","\n","\n","def remove_special_characters(text):\n","  pattern = r'[^a-zA-z\\s]'\n","  text = re.sub(pattern,'',text)\n","  return text\n","\n","\n","def low_level_preproc(text):\n","  text = strip_html(text)\n","  text = remove_between_square_brackets(text)\n","  text = remove_special_characters(text)\n","  return text\n","\n","#Apply function on review column\n","imdb_data['review'] = imdb_data['review'].apply(low_level_preproc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"orEVYliBvEmq"},"source":["all_stopwords = set(stopwords.words(\"english\"))\n","\n","def remove_stop_words(full_text_line):\n","  tokens = full_text_line.split()\n","  tokens = [tok for tok in tokens if tok not in all_stopwords]\n","\n","  return \" \".join(tokens)\n","\n","\n","def lemmatize(text):\n","  wnl= WordNetLemmatizer()\n","  lemas = [wnl.lemmatize(word) for word in text.split()]\n","\n","  return \" \".join(lemas)\n","\n","\n","def high_level_preproc(text):\n","  text = remove_stop_words(text)\n","  return lemmatize(text)\n","\n","\n","#Apply function on review column\n","imdb_data['review'] = imdb_data['review'].str.lower()\n","imdb_data['review'] = imdb_data['review'].apply(high_level_preproc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgyBKqd6uhLz"},"source":["#split the dataset  \n","#train dataset\n","train_reviews = imdb_data.review[:40000]\n","train_sentiments = imdb_data.sentiment[:40000]\n","\n","#test dataset\n","test_reviews = imdb_data.review[40000:]\n","test_sentiments = imdb_data.sentiment[40000:]\n","\n","\n","print(\"Train set:\", train_reviews.shape, train_sentiments.shape)\n","print(\"Test set:\", test_reviews.shape, test_sentiments.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6QZ6OiUBrzbM"},"source":["# Vocabulario y Encoding\n","\n","Vamos a crear un volcabulario para el problema, de este modo podemos representar cada palabra con un entero único. Esto nos va a permitir representar una review como una lista de ints (que luego el modelo va a mapear a word embeddings!).\n","\n","Una cosa a tener en cuenta es que vamos a querer agregar padding a nuestros inputs (para que todas las reviews tengan el mismo largo), para esto vamos a usar el 0, por lo que las palabras de nuestro vocabulario deben empezar en 1.\n","\n"]},{"cell_type":"code","metadata":{"id":"Nu2ZDb1Ury0b"},"source":["def make_vocab(all_texts, max_vocab_size, oov_token=\"<OOV>\"):\n","  # Count the number of occurrences of each word\n","\n","  # Create vocab containing max_vocab_size tokens\n","  # Add the out of vocabulary at the end\n","\n","  # Map from word to int index in vocab\n","\n","  return vocab_to_int"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ALBSYzBx6ag"},"source":["vocab_mapping = make_vocab(train_reviews, 100_000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gnQOZt1PyNYW"},"source":["Ahora vamos a implementar una funcion que transforma un string con la review en una lista de enteros con la posiscion de cada una de nuestras palabras en el vocabulario. Si una palabra no está en el vocabulario usamos el indice para`\"<OOV>\"`"]},{"cell_type":"code","metadata":{"id":"XfoHxQwExN8W"},"source":["def get_review_features(text, word_to_idx):\n","\n","  return indices"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mQZ0dRi0A3t"},"source":["Lo siguiente es implementar padding de las sentencias, si bien los modelos son capaces de trabajar con secuencias de cualquier largo queremos que el tiempo de entrenamiento e inferencia esté controlado y no dependa del largo de los inputs. Como una pequeña optimizacion vamos a hacer **left padding**, es decir, agregar 0s a la izquierda de una secuencia hasta alcanzar `max_sequence_length` elementos.\n","\n","Agregar ceros a la izquierda ayuda a los modelos a aprender de los datos ya que la informacion valiosa aparece al final de la secuencia y no tiene que recordar 3 palabras en luego de haber visto 100 ceros..\n","\n","Ejemplo, la secuencia\n","\n","`[117, 18, 128]`\n","\n"," Quedaría:\n","\n","`[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]`\n","\n","En lugar de \n","\n","`[117, 18, 128, 0, 0, 0, 0, 0, 0, 0] ` Forzando al modelo a recordar los 3 primeros inputs para poder predecir algo.\n"]},{"cell_type":"code","metadata":{"id":"8DY_mllOqOvX"},"source":["def pad_features(review_ints, sequence_length):\n","\n","  return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pz43OMIV2ejT"},"source":["def get_review_representation(review_text, word_to_idx, max_sequence_length):\n","  return ?"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lXrRSrbX2AqF"},"source":["# Transformando los textos a vectores"]},{"cell_type":"code","metadata":{"id":"JaSoT9Vu2RAX"},"source":["MAX_SEQUENCE_LENGTH = 100\n","\n","train_vectors = train_reviews.apply(lambda x: get_review_representation(x, vocab_mapping, MAX_SEQUENCE_LENGTH))\n","test_vectors = test_reviews.apply(lambda x: get_review_representation(x, vocab_mapping, MAX_SEQUENCE_LENGTH))\n","\n","train_vectors = np.array([vec for vec in train_vectors])\n","test_vectors = np.array([vec for vec in test_vectors])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6uOvDvL43-L"},"source":["# Codigo de entrenamiento e inferencia \n","\n","Same old..\n"]},{"cell_type":"code","metadata":{"id":"Bc8LaIv149Hg"},"source":["def train_epoch(training_model, loader, criterion, optim):\n","    training_model.train()\n","    epoch_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    for data, labels in loader:\n","      all_labels.extend(labels.numpy())  \n","\n","      optim.zero_grad()\n","\n","      predictions = training_model(data.to(DEVICE))\n","      all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","\n","      loss = criterion(predictions, labels.to(DEVICE))\n","      \n","      loss.backward()\n","      optim.step()\n","\n","      epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n","\n","\n","def validation_epoch(val_model, loader, criterion):\n","    val_model.eval()\n","    epoch_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","    \n","    with torch.no_grad():\n","      for data, labels in loader:\n","        all_labels.extend(labels.numpy())  \n","\n","        predictions = val_model(data.to(DEVICE))\n","        all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n","\n","        loss = criterion(predictions, labels.to(DEVICE))\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader), accuracy_score(all_labels, all_predictions) * 100\n","  \n","\n","def train_model(model, train_loader, test_loader, criterion, optim, number_epochs):\n","  train_history = []\n","  test_history = []\n","  accuracy_history = []\n","\n","  for epoch in range(number_epochs):\n","      start_time = time.time()\n","\n","      train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n","      train_history.append(train_loss)\n","      print(\"Training epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n","            .format(epoch + 1, train_loss, train_acc, time.time() - start_time))\n","\n","      start_time = time.time()\n","      test_loss, acc = validation_epoch(model, test_loader, criterion)\n","      test_history.append(test_loss)\n","      accuracy_history.append(acc)\n","      print(\"Validation epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n","            .format(epoch + 1, test_loss, acc, time.time() - start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkFS0KDj5AKp"},"source":["# Modelo\n"]},{"cell_type":"code","metadata":{"id":"VmHNNvSfHLq_"},"source":["class SentimentRNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","        super(SentimentRNN, self).__init__()\n","        # Necesitan una capa de Embedding y una RNN.\n","        # El resto (y sus hyperparametros) corre por su cuenta.\n","        # 2 outputs\n","\n","    def forward(self, x):\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sahj6WEl5Tii"},"source":["# Entrenamiento"]},{"cell_type":"code","metadata":{"id":"iw76sUmhyxAw"},"source":["train_targets = torch.Tensor(train_sentiments.to_numpy()).long()\n","train_dataset = TensorDataset(torch.LongTensor(train_vectors), train_targets) \n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=2)\n","\n","test_targets = torch.Tensor(test_sentiments.to_numpy()).long()\n","test_dataset = TensorDataset(torch.LongTensor(test_vectors), test_targets) \n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqW4_B46HZTt"},"source":["loss_function = nn.CrossEntropyLoss().to(DEVICE)\n","BATCH_SIZE = 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssVUQ3CL-V6-"},"source":["# Instanciar un modelo\n","# Crear optimizador\n","# Entrenar (Una RNN normal debería poder superar 60% de accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h-Dw-KmYDAjF"},"source":["# Mejoras en el modelo\n","\n","\n","\n","1. Podemos mejorar la performance si usamos una GRU o una LSTM ?\n","2. Que pasa si usamos celdas **bidireccionales** ?\n","3. Que pasa si aumentamos el numero de **capas** de nuestras celdas recurrentes \n","?\n","4. Y si usamos vectores preentrenados (W2V, GloVe) ?\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"6e7iF06_Dcdk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rtsn-YNxDcgB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"veuzMIyjDchz"},"source":[""],"execution_count":null,"outputs":[]}]}